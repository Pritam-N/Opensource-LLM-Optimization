{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64dbd71b",
   "metadata": {},
   "source": [
    "# Benchmarking\n",
    "\n",
    "The purpose of this jupyter notebook is to load original/optimized models to device(CPU/GPU) and run benchmarks like inference throughput and model size.\n",
    "\n",
    "Model Loading: Load the Tiny-llama-1b-chat model using Hugging Face's transformers library. This involves retrieving the pre-trained model and its tokenizer, which are essential for processing input data and generating responses. On the second part of this notebook the pretrained models are loaded using **OVModelForCausalLM** API.\n",
    "\n",
    "Next, moving to a Python environment, we import the appropriate modules and  download the original model as well as its processor.â€‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c7257359-27e6-4866-8b99-057d15eed8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import SUPPORTED_LLM_MODELS\n",
    "import ipywidgets as widgets\n",
    "from optimum.intel.openvino import OVModelForCausalLM\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "from transformers import AutoModelForCausalLM, AutoConfig\n",
    "from optimum.intel import OVQuantizer\n",
    "import openvino as ov\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import torch\n",
    "import logging\n",
    "import nncf\n",
    "import gc\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    StoppingCriteria,\n",
    "    StoppingCriteriaList,\n",
    "    TextIteratorStreamer,\n",
    ")\n",
    "from converter import converters, register_configs\n",
    "\n",
    "register_configs()\n",
    "core = ov.Core()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedcccf0",
   "metadata": {},
   "source": [
    "Below is a sampele of prompt and configs used for seting up the LLM and generating responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea74ac2e-c63f-4e02-b2d0-939a28f214e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_configuration = {'model_id': 'TinyLlama/TinyLlama-1.1B-Chat-v1.0',\n",
    " 'remote': False,\n",
    " 'start_message': \"<|system|>\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\nIf a question does not make any sense or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.</s>\\n\",\n",
    " 'history_template': '<|user|>\\n{user}</s> \\n<|assistant|>\\n{assistant}</s> \\n',\n",
    " 'current_message_template': '<|user|>\\n{user}</s> \\n<|assistant|>\\n{assistant}',\n",
    " 'prompt_template': \"<|system|> You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.</s>\\n        <|user|>\\n        Question: {question} \\n        Context: {context} \\n        Answer: </s>\\n        <|assistant|>\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9fc09b",
   "metadata": {},
   "source": [
    "Below code block is used to read the pre-trained TinyLlama/TinyLlama-1.1B-Chat-v1.0 from HF using AutoModelForCausalLM APIs. Once the model is loaded, the total memory size is calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed394c94-2f37-4af7-9c4f-20cbc554fb7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate model size in memory: 4218.35 MB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "model_fp32 = AutoModelForCausalLM.from_pretrained(model_configuration['model_id'])\n",
    "\n",
    "def model_memory_size(model):\n",
    "    param_size = 0\n",
    "    for param in model.parameters():\n",
    "        param_size += param.nelement() * param.element_size()\n",
    "    buffer_size = 0\n",
    "    for buffer in model.buffers():\n",
    "        buffer_size += buffer.nelement() * buffer.element_size()\n",
    "    total_size = param_size + buffer_size\n",
    "    return total_size / (1024 ** 2)  # Convert to megabytes\n",
    "\n",
    "size_in_mb = model_memory_size(model_fp32)\n",
    "print(f\"Approximate model size in memory: {size_in_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "239d1b6e-94c4-4fbd-a852-3332454c9c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bba397",
   "metadata": {},
   "source": [
    "Setting up the model path tha will be later used to load the quantized models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cee83c87-c7ab-4c0c-8ad8-1aeb862c1527",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp16_model_dir = Path('tiny-llama-1b-chat/FP16')\n",
    "int8_model_dir = Path('tiny-llama-1b-chat/INT8_compressed_weights')\n",
    "int4_model_dir = Path('tiny-llama-1b-chat/INT4_compressed_weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553c96ee",
   "metadata": {},
   "source": [
    "Let's compare model size for different compression types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f36fa91-1c85-4f9e-9cd0-18830c3a4ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of FP16 model is 2098.68 MB\n",
      "Size of model with INT8 compressed weights is 1050.55 MB\n",
      "Compression rate for INT8 model: 1.998\n",
      "Size of model with INT4 compressed weights is 696.51 MB\n",
      "Compression rate for INT4 model: 3.013\n"
     ]
    }
   ],
   "source": [
    "fp16_weights = fp16_model_dir / \"openvino_model.bin\"\n",
    "int8_weights = int8_model_dir / \"openvino_model.bin\"\n",
    "int4_weights = int4_model_dir / \"openvino_model.bin\"\n",
    "\n",
    "if fp16_weights.exists():\n",
    "    print(f\"Size of FP16 model is {fp16_weights.stat().st_size / 1024 / 1024:.2f} MB\")\n",
    "for precision, compressed_weights in zip([8, 4], [int8_weights, int4_weights]):\n",
    "    if compressed_weights.exists():\n",
    "        print(\n",
    "            f\"Size of model with INT{precision} compressed weights is {compressed_weights.stat().st_size / 1024 / 1024:.2f} MB\"\n",
    "        )\n",
    "    if compressed_weights.exists() and fp16_weights.exists():\n",
    "        print(\n",
    "            f\"Compression rate for INT{precision} model: {fp16_weights.stat().st_size / compressed_weights.stat().st_size:.3f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e1972e6-6dc0-43ea-befd-cb77a35b8672",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78f137ab447742c1be6cc06ab64efbed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Device:', options=('CPU', 'AUTO'), value='CPU')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = widgets.Dropdown(\n",
    "    options=core.available_devices + [\"AUTO\"],\n",
    "    value=\"CPU\",\n",
    "    description=\"Device:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce8218e",
   "metadata": {},
   "source": [
    "# Model Initialization and Throughput Calculation\n",
    "\n",
    "This notebook demonstrates the initialization of an OpenVINO-optimized Large Language Model (LLM) and the calculation of its throughput. We use a closure to encapsulate model initialization, ensuring that the model is loaded only once when its directory changes. This approach enhances efficiency by reusing the initialized model for subsequent throughput calculations without reloading it. \n",
    "\n",
    "## Initialization with Closure\n",
    "\n",
    "A closure, `model_initializer`, is defined to create a function that remembers the last initialized model and tokenizer. This avoids unnecessary reinitialization and leverages caching for improved performance.\n",
    "\n",
    "## Throughput Calculation\n",
    "Define a function, calculate_throughput, which calculates the model's throughput. This function uses the previously defined closure to initialize the model if necessary and then measures how quickly the model can generate responses to a sample input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e40ef8ba-ab8e-4557-a636-4d38411091f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoConfig\n",
    "from time import perf_counter\n",
    "\n",
    "# Assuming OVModelForCausalLM and other necessary imports are defined elsewhere\n",
    "\n",
    "def model_initializer():\n",
    "    initialized_model_dir = None\n",
    "    model = None\n",
    "    tok = None\n",
    "\n",
    "    def initialize_model_if_needed(model_dir, model_name, model_configuration, device, tokenizer_kwargs):\n",
    "        nonlocal initialized_model_dir, model, tok\n",
    "\n",
    "        # Check if the model_dir has changed or if the model hasn't been initialized yet\n",
    "        if model_dir != initialized_model_dir or model is None:\n",
    "            print(f\"Initializing model from {model_dir}...\")\n",
    "            class_key = model_name.split(\"-\")[0]\n",
    "            tok = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "            ov_config = {\"PERFORMANCE_HINT\": \"LATENCY\", \"NUM_STREAMS\": \"1\", \"CACHE_DIR\": \"\"}\n",
    "            model_class = (\n",
    "                OVModelForCausalLM\n",
    "                if not model_configuration[\"remote\"]\n",
    "                else model_classes[class_key]\n",
    "            )\n",
    "            model = model_class.from_pretrained(\n",
    "                model_dir,\n",
    "                device=device,\n",
    "                ov_config=ov_config,\n",
    "                config=AutoConfig.from_pretrained(model_dir, trust_remote_code=True),\n",
    "                trust_remote_code=True,\n",
    "            )\n",
    "            initialized_model_dir = model_dir\n",
    "        else:\n",
    "            print(f\"Using cached model from {model_dir}.\")\n",
    "\n",
    "        return model, tok\n",
    "\n",
    "    return initialize_model_if_needed\n",
    "\n",
    "# Create a closure function that remembers the last initialized model\n",
    "initialize_model = model_initializer()\n",
    "\n",
    "def calculate_throughput(model_dir, model_name, quantization_type, model_configuration, device, tokenizer_kwargs):\n",
    "    # Use the closure to initialize the model if needed\n",
    "    ov_model, tok = initialize_model(model_dir, model_name, model_configuration, device, tokenizer_kwargs)\n",
    "\n",
    "    input_tokens = tok('a sometimes tedious film.', return_tensors=\"pt\", **tokenizer_kwargs)\n",
    "\n",
    "    # Assuming TextIteratorStreamer and other necessary configurations are defined\n",
    "    streamer = TextIteratorStreamer(tok, timeout=30.0, skip_prompt=True, skip_special_tokens=True)\n",
    "    generate_kwargs = dict(\n",
    "        input_ids=input_tokens.input_ids,\n",
    "        max_new_tokens=256,\n",
    "        temperature=0.1,\n",
    "        do_sample=0.1 > 0.0,\n",
    "        top_p=1,\n",
    "        top_k=50,\n",
    "        repetition_penalty=1.1,\n",
    "        streamer=streamer,\n",
    "    )\n",
    "\n",
    "    start = perf_counter()\n",
    "    answer = ov_model.generate(**generate_kwargs)\n",
    "    end = perf_counter()\n",
    "    duration = end - start\n",
    "    print(f\"Throughput of {quantization_type} {model_name} is {len(answer[0]) / duration}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9df29bc5-107e-4d8e-b97b-b03426ef65b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_kwargs = dict(\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08e1e88",
   "metadata": {},
   "source": [
    "This code block is designed to showcase how efficiently our model can process and respond to natural language inputs, providing valuable insights into the performance optimizations achieved through model quantization and OpenVINO integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "53d1bf4f-35e0-4440-bb4f-a86f9570638d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model from tiny-llama-1b-chat/FP16...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used along with export=True. It will be ignored.\n",
      "Compiling the model to CPU ...\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Throughput of FP16:  TinyLlama/TinyLlama-1.1B-Chat-v1.0 is 2.4456816906266847\n",
      "Using cached model from tiny-llama-1b-chat/FP16.\n",
      "Throughput of FP16:  TinyLlama/TinyLlama-1.1B-Chat-v1.0 is 5.727936268042223\n"
     ]
    }
   ],
   "source": [
    "calculate_throughput(fp16_model_dir, model_configuration[\"model_id\"], 'FP16: ', model_configuration, device.value, tokenizer_kwargs)\n",
    "calculate_throughput(fp16_model_dir, model_configuration[\"model_id\"], 'FP16: ', model_configuration, device.value, tokenizer_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "962729ba-6cd0-4ffe-9629-d94bd0226605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model from tiny-llama-1b-chat/INT8_compressed_weights...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used along with export=True. It will be ignored.\n",
      "Compiling the model to CPU ...\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Throughput of INT8:  TinyLlama/TinyLlama-1.1B-Chat-v1.0 is 3.8877829410108555\n",
      "Using cached model from tiny-llama-1b-chat/INT8_compressed_weights.\n",
      "Throughput of INT8:  TinyLlama/TinyLlama-1.1B-Chat-v1.0 is 20.9805502640221\n"
     ]
    }
   ],
   "source": [
    "calculate_throughput(int8_model_dir, model_configuration[\"model_id\"], 'INT8: ', model_configuration, device.value, tokenizer_kwargs)\n",
    "calculate_throughput(int8_model_dir, model_configuration[\"model_id\"], 'INT8: ', model_configuration, device.value, tokenizer_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c69f4087-c9e3-4a53-bf95-daf276494f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model from tiny-llama-1b-chat/INT4_compressed_weights...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used along with export=True. It will be ignored.\n",
      "Compiling the model to CPU ...\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Throughput of INT4:  TinyLlama/TinyLlama-1.1B-Chat-v1.0 is 8.845335131923164\n",
      "Using cached model from tiny-llama-1b-chat/INT4_compressed_weights.\n",
      "Throughput of INT4:  TinyLlama/TinyLlama-1.1B-Chat-v1.0 is 25.990472594905917\n"
     ]
    }
   ],
   "source": [
    "calculate_throughput(int4_model_dir, model_configuration[\"model_id\"], 'INT4: ', model_configuration, device.value, tokenizer_kwargs)\n",
    "calculate_throughput(int4_model_dir, model_configuration[\"model_id\"], 'INT4: ', model_configuration, device.value, tokenizer_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fb9f99aa-4de2-420d-9d0b-21b3c59fc6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Throughput of ORIGINAL: TinyLlama/TinyLlama-1.1B-Chat-v1.0 is 4.859636410956347\n"
     ]
    }
   ],
   "source": [
    "tok = AutoTokenizer.from_pretrained(model_configuration['model_id'])\n",
    "model_fp32 = AutoModelForCausalLM.from_pretrained(model_configuration['model_id'])\n",
    "input_tokens = tok('a sometimes tedious film.', return_tensors=\"pt\", **tokenizer_kwargs)\n",
    "\n",
    "# Assuming TextIteratorStreamer and other necessary configurations are defined\n",
    "streamer = TextIteratorStreamer(tok, timeout=30.0, skip_prompt=True, skip_special_tokens=True)\n",
    "generate_kwargs = dict(\n",
    "    input_ids=input_tokens.input_ids,\n",
    "    max_new_tokens=256,\n",
    "    temperature=0.1,\n",
    "    do_sample=0.1 > 0.0,\n",
    "    top_p=1,\n",
    "    top_k=50,\n",
    "    repetition_penalty=1.1,\n",
    "    streamer=streamer,\n",
    ")\n",
    "\n",
    "start = perf_counter()\n",
    "answer = model_fp32.generate(**generate_kwargs)\n",
    "end = perf_counter()\n",
    "duration = end - start\n",
    "print(f\"Throughput of ORIGINAL: {model_configuration['model_id']} is {len(answer[0]) / duration}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "74baeb29-a363-429a-bb6f-25ba9c95bbcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Throughput of ORIGINAL: TinyLlama/TinyLlama-1.1B-Chat-v1.0 is 5.9102147189626475\n"
     ]
    }
   ],
   "source": [
    "input_tokens = tok('a sometimes tedious film.', return_tensors=\"pt\", **tokenizer_kwargs)\n",
    "\n",
    "# Assuming TextIteratorStreamer and other necessary configurations are defined\n",
    "streamer = TextIteratorStreamer(tok, timeout=30.0, skip_prompt=True, skip_special_tokens=True)\n",
    "generate_kwargs = dict(\n",
    "    input_ids=input_tokens.input_ids,\n",
    "    max_new_tokens=256,\n",
    "    temperature=0.1,\n",
    "    do_sample=0.1 > 0.0,\n",
    "    top_p=1,\n",
    "    top_k=50,\n",
    "    repetition_penalty=1.1,\n",
    "    streamer=streamer,\n",
    ")\n",
    "\n",
    "start = perf_counter()\n",
    "answer = model_fp32.generate(**generate_kwargs)\n",
    "end = perf_counter()\n",
    "duration = end - start\n",
    "print(f\"Throughput of ORIGINAL: {model_configuration['model_id']} is {len(answer[0]) / duration}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0886e4-d8d1-4735-ab34-7c496f6ad0d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
